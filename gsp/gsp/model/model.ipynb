{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, cast, Dict\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from gsp.utils.column_transformer_wrapper import ColumnTransformerWrapper\n",
    "from gsp.utils.group_shifts import make_mw_in_groups, make_shift_in_groups\n",
    "from gsp.utils.date_utils import get_nth_previous_working_date\n",
    "from gsp.utils.notebook_utils import show\n",
    "from data import SCRAPED_STOCK_FILE_PATH, OUTPUT_DIR_PATH, save_output, load_output\n",
    "from lib.logger.setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "N_STEPS: int = 21\n",
    "N_OPTIMIZE_TRIALS: int = -1  # --- NOTICE --- Negative value means no optimization\n",
    "DAYS_BACK_TO_CONSIDER: int = 5 * 252\n",
    "LABEL_FEATURES: List[str] = [\"Year\"]\n",
    "CATEGORICAL_FEATURES: List[str] = [\"DayOfWeek\", \"AreaCat\"]\n",
    "SHIFT_LIST: List[int] = [1, 2, 3]\n",
    "MWM_LIST: List[int] = [5, 10, 15]\n",
    "HYPER_PARAMS: Dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    return cast(float, mean_squared_log_error(y_true, y_pred) ** 0.5)\n",
    "\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        SCRAPED_STOCK_FILE_PATH,\n",
    "        dtype={\n",
    "            \"Date\": \"period[D]\",\n",
    "            \"Open\": \"float\",\n",
    "            \"High\": \"float\",\n",
    "            \"Low\": \"float\",\n",
    "            \"Close\": \"float\",\n",
    "            \"Volume\": \"int\",\n",
    "            \"Area\": \"category\",\n",
    "            \"Name\": \"category\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy(deep=True)\n",
    "    periods = pd.date_range(\n",
    "        start=df[\"Date\"].min().to_timestamp().date(), end=df[\"Date\"].max().to_timestamp().date(), freq=\"B\"\n",
    "    )\n",
    "    periods_df = pd.DataFrame({\"Date\": periods}, dtype=\"period[D]\")\n",
    "\n",
    "    clean_data = (\n",
    "        cast(\n",
    "            pd.DataFrame,\n",
    "            periods_df.set_index(\"Date\")\n",
    "            .join(df.set_index(\"Date\"))\n",
    "            .set_index(\"Name\", append=True)\n",
    "            .sort_index()\n",
    "            .unstack(\"Name\")\n",
    "            .ffill()\n",
    "            .bfill()  # --- NOTICE --- This is a fix so that the single problem approach works. It is not the best way to handle missing data.\n",
    "            .stack(\"Name\", future_stack=True),  # type: ignore\n",
    "        )\n",
    "        .reset_index()\n",
    "        .dropna(subset=[\"Name\"])\n",
    "        .set_index([\"Date\", \"Name\"])\n",
    "    )\n",
    "\n",
    "    return clean_data\n",
    "\n",
    "\n",
    "def engineer_features(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_features: List[str] = [],\n",
    "    shift_list: List[int] = [],\n",
    "    mwm_list: List[int] = [],\n",
    "    label_features: List[str] = [],\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    cat_features_to_use: List[str] = [*categorical_features, *label_features]\n",
    "\n",
    "    if \"DayOfWeek\" in cat_features_to_use:\n",
    "        df[\"DayOfWeek\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).dayofweek\n",
    "\n",
    "    if \"Month\" in cat_features_to_use:\n",
    "        df[\"Month\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).month\n",
    "\n",
    "    if \"Year\" in cat_features_to_use:\n",
    "        df[\"Year\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).year\n",
    "\n",
    "    if \"WeekOfYear\" in cat_features_to_use:\n",
    "        df[\"WeekOfYear\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).week  # type: ignore\n",
    "\n",
    "    if \"DayOfMonth\" in cat_features_to_use:\n",
    "        df[\"DayOfMonth\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).day\n",
    "\n",
    "    if \"Quarter\" in cat_features_to_use:\n",
    "        df[\"Quarter\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).quarter\n",
    "\n",
    "    if \"AreaCat\" in cat_features_to_use:\n",
    "        df[\"AreaCat\"] = df[\"Area\"]\n",
    "\n",
    "    grouped_lags: List[pd.DataFrame | pd.Series] = [\n",
    "        make_shift_in_groups(df, groupby=[\"Name\"], column=\"Close\", shift=shift_list),\n",
    "        make_mw_in_groups(df, groupby=[\"Name\"], column=\"Close\", window=mwm_list),\n",
    "    ]\n",
    "\n",
    "    df_grouped_lags = df.join(grouped_lags, how=\"left\")\n",
    "\n",
    "    return df_grouped_lags\n",
    "\n",
    "\n",
    "def process_data(\n",
    "    df: pd.DataFrame,\n",
    "    n_steps: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str] = [],\n",
    "    days_back_to_consider: int | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    \"\"\"First date where all stocks have data\"\"\"\n",
    "    first_all_valid_date: datetime.date = (\n",
    "        cast(pd.Period, df.unstack(\"Name\").dropna().first_valid_index()).to_timestamp().date()\n",
    "    )\n",
    "\n",
    "    latest_date: datetime.date = cast(pd.Period, df.index.get_level_values(\"Date\").max()).to_timestamp().date()\n",
    "\n",
    "    earliest_date: datetime.date = first_all_valid_date\n",
    "\n",
    "    starting_date_to_consider: datetime.date | None = None\n",
    "\n",
    "    if days_back_to_consider is not None:\n",
    "        starting_date_to_consider = get_nth_previous_working_date(n=days_back_to_consider, date=latest_date)\n",
    "\n",
    "    if starting_date_to_consider is not None and starting_date_to_consider < first_all_valid_date:\n",
    "        logger.warning(\n",
    "            f\"WARNING: starting_date_to_consider ({starting_date_to_consider}) is before the first date where all stocks have data. Using {first_all_valid_date} instead.\"\n",
    "        )\n",
    "    elif starting_date_to_consider is not None:\n",
    "        earliest_date = starting_date_to_consider\n",
    "\n",
    "    logger.info(f\"Earliest date: {earliest_date.isoformat()}\")\n",
    "\n",
    "    ctw = ColumnTransformerWrapper(\n",
    "        transformers=[\n",
    "            (\"onehot\", OneHotEncoder(drop=\"first\"), categorical_features),\n",
    "            (\"label\", LabelEncoder(), label_features),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    df_from_earliest = df.loc[earliest_date.isoformat() :]  # type: ignore\n",
    "    X = ctw.fit_transform(df_from_earliest.drop(columns=[\"Adj Close\", \"Volume\", \"High\", \"Low\", \"Open\", \"Area\"]))\n",
    "    y = make_shift_in_groups(df, groupby=[\"Name\"], column=\"Close\", shift=[-i for i in range(1, n_steps + 1)])\n",
    "\n",
    "    y, X = y.align(X.dropna(), axis=0, join=\"inner\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    n_steps: int,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"A function that splits the time series data into train and test sets in regards\n",
    "    to the number of forecasting steps. It will test the model on the last 2*n_steps.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Features dataframe\n",
    "        y (pd.DataFrame): Target dataframe\n",
    "        n_steps (int): Number of steps of the forecasting task\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    latest_date = cast(pd.Period, X.index.get_level_values(\"Date\").max()).to_timestamp().date()\n",
    "    test_end_date = get_nth_previous_working_date(n=n_steps, date=latest_date)\n",
    "    test_start_date = get_nth_previous_working_date(n=2 * n_steps - 1, date=latest_date)\n",
    "    train_end_date = get_nth_previous_working_date(n=2 * n_steps, date=latest_date)\n",
    "\n",
    "    X_train = X.loc[: train_end_date.isoformat()]  # type: ignore\n",
    "    y_train = y.loc[: train_end_date.isoformat()]  # type: ignore\n",
    "    X_test = X.loc[test_start_date.isoformat() : test_end_date.isoformat()]  # type: ignore\n",
    "    y_test = y.loc[test_start_date.isoformat() : test_end_date.isoformat()]  # type: ignore\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def create_model(**hyper_params):\n",
    "    model = RegressorChain(XGBRegressor(**hyper_params))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_last_prediction_to_output(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    latest_date = cast(pd.Period, df.index.get_level_values(\"Date\").max()).to_timestamp().date()\n",
    "\n",
    "    y_latest = cast(pd.DataFrame, df.loc[latest_date.isoformat()]).rename(\n",
    "        columns={\n",
    "            f\"Close_lead_{i}\": get_nth_previous_working_date(n=-i, date=latest_date).isoformat()\n",
    "            for i in range(1, len(df.columns) + 1)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    y_output = (\n",
    "        y_latest.reset_index()\n",
    "        .melt(id_vars=[\"Name\"], var_name=\"Date\", value_name=\"Close\")\n",
    "        .astype(\n",
    "            {\n",
    "                \"Date\": \"period[D]\",\n",
    "                \"Name\": \"category\",\n",
    "                \"Close\": \"float\",\n",
    "            }\n",
    "        )\n",
    "        .set_index([\"Date\", \"Name\"])\n",
    "    )\n",
    "\n",
    "    return y_output\n",
    "\n",
    "\n",
    "def solve(\n",
    "    hyper_params: Dict,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_query: pd.DataFrame,\n",
    "    single_problem_approach: bool = False,\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "    default_model = create_model(**hyper_params=hyper_params)\n",
    "    run_params = default_model.get_params()\n",
    "\n",
    "    y_pred = pd.DataFrame()\n",
    "    if single_problem_approach is False:\n",
    "        logger.info(\"Training and predicting for all stocks\")\n",
    "        model = create_model(**hyper_params)\n",
    "        model.fit(X, y)\n",
    "        y_pred = pd.DataFrame(model.predict(X_query), index=X_query.index, columns=y.columns)  # type: ignore\n",
    "    else:\n",
    "        unique_stock_names = X.index.get_level_values(\"Name\").unique()\n",
    "        for i, name in enumerate(unique_stock_names):\n",
    "            logger.info(f\"Training and predicting for {name} | {i + 1}/{len(unique_stock_names)}\")\n",
    "            X_single = cast(pd.DataFrame, X.loc[X.index.get_level_values(\"Name\") == name])\n",
    "            y_single = cast(pd.DataFrame, y.loc[y.index.get_level_values(\"Name\") == name])\n",
    "            X_query_single = cast(pd.DataFrame, X_query.loc[X_query.index.get_level_values(\"Name\") == name])\n",
    "            model = create_model(**hyper_params)\n",
    "            model.fit(X_single, y_single)\n",
    "            y_pred_single = pd.DataFrame(model.predict(X_query_single), index=X_query_single.index, columns=y_single.columns)  # type: ignore\n",
    "            y_pred = pd.concat([y_pred, y_pred_single], axis=0)\n",
    "\n",
    "    y_pred = y_pred.sort_index().clip(lower=0)\n",
    "    logger.info(\"Prediction done\")\n",
    "    return y_pred, run_params\n",
    "\n",
    "def optimize_with_optuna(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "    single_problem_approach: bool,\n",
    "    n_optimize_trials: int,\n",
    ") -> Tuple[float, Dict]:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=f\"optuna_optimization_{'single_approach' if single_problem_approach else \"multi_approach\"}\",\n",
    "    )\n",
    "\n",
    "    def optuna_optimize_objective(trial: optuna.Trial) -> float:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1200, step=50),  # Number of trees in the ensemble\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),  # Maximum depth of each tree\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),  # Learning rate\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),  # Subsample ratio of the training instances\n",
    "            \"colsample_bytree\": trial.suggest_float(\n",
    "                \"colsample_bytree\", 0.5, 1.0\n",
    "            ),  # Subsample ratio of columns when constructing each tree\n",
    "            \"gamma\": trial.suggest_float(\n",
    "                \"gamma\", 0.01, 10.0, log=True\n",
    "            ),  # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 100.0, log=True),  # L1 regularization term on weights\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 100.0, log=True),  # L2 regularization term on weights\n",
    "            \"min_child_weight\": trial.suggest_float(\n",
    "                \"min_child_weight\", 1, 100, log=True\n",
    "            ),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "        }\n",
    "\n",
    "        y_pred, run_params = solve(\n",
    "            hyper_params=params,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            X_query=X_test,\n",
    "            single_problem_approach=single_problem_approach,\n",
    "        )\n",
    "        rmsle = root_mean_squared_log_error(y_test, y_pred)\n",
    "        return rmsle\n",
    "\n",
    "    study.optimize(optuna_optimize_objective, n_trials=n_optimize_trials)\n",
    "    best_params = study.best_params\n",
    "    best_rmsle = study.best_value\n",
    "\n",
    "    return best_rmsle, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_test_run(\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict = {},\n",
    "    n_optimize_trials: int = -1,\n",
    "    single_problem_approach: bool = False,\n",
    "    combined: bool = False,\n",
    "):\n",
    "\n",
    "    stocks = load_data()\n",
    "    clean = clean_data(stocks)\n",
    "    features = engineer_features(\n",
    "        clean, categorical_features, shift_list=shift_list, mwm_list=mwm_list, label_features=label_features\n",
    "    )\n",
    "    X, y = process_data(features, n_steps, categorical_features, days_back_to_consider=days_back_to_consider)\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, n_steps)\n",
    "\n",
    "    # --- Setup for the test run ---\n",
    "    final_rmsle: float = 0.0\n",
    "    final_y_pred: pd.DataFrame = pd.DataFrame()\n",
    "    final_hyper_params: Dict = {}\n",
    "\n",
    "    # --- Training and predicting ---\n",
    "    if combined is False:\n",
    "        y_default_pred, run_params = solve(\n",
    "            hyper_params=hyper_params,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            X_query=X_test,\n",
    "            single_problem_approach=single_problem_approach,\n",
    "        )\n",
    "    else:\n",
    "        y_single_pred, run_params = solve(\n",
    "            hyper_params=hyper_params,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            X_query=X_test,\n",
    "            single_problem_approach=False,\n",
    "        )\n",
    "        y_multi_pred, run_params = solve(\n",
    "            hyper_params=hyper_params,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            X_query=X_test,\n",
    "            single_problem_approach=True,\n",
    "        )\n",
    "\n",
    "        y_default_pred = (y_single_pred + y_multi_pred) / 2\n",
    "\n",
    "    rmsle_default = root_mean_squared_log_error(y_test, y_default_pred)\n",
    "\n",
    "    final_rmsle = rmsle_default\n",
    "    final_y_pred = y_default_pred\n",
    "    final_hyper_params = hyper_params\n",
    "\n",
    "    if n_optimize_trials > 0:\n",
    "        optuna_rmsle, optuna_hyper_params = optimize_with_optuna(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            single_problem_approach=single_problem_approach,\n",
    "            n_optimize_trials=n_optimize_trials,\n",
    "        )\n",
    "\n",
    "        if optuna_rmsle < final_rmsle:\n",
    "\n",
    "            final_hyper_params = optuna_hyper_params\n",
    "            final_rmsle = optuna_rmsle\n",
    "            final_y_pred = solve(\n",
    "                hyper_params=optuna_hyper_params,\n",
    "                X=X_train,\n",
    "                y=y_train,\n",
    "                X_query=X_test,\n",
    "                single_problem_approach=single_problem_approach,\n",
    "            )\n",
    "\n",
    "    y_output = convert_last_prediction_to_output(final_y_pred)\n",
    "    return final_rmsle, y_output, final_hyper_params\n",
    "\n",
    "\n",
    "def save_test_logs(\n",
    "    rmsle: float,\n",
    "    y_output: pd.DataFrame,\n",
    "    hyper_params: Dict,\n",
    "    single_problem_approach: bool,\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    ") -> None:\n",
    "    test_history_log_df = load_output(\"test_history_log.csv\")\n",
    "    test_prediction_date = get_nth_previous_working_date(\n",
    "        n=1, date=cast(pd.Period, y_output.index.get_level_values(\"Date\").min()).to_timestamp().date()\n",
    "    )\n",
    "    current_input_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Version\": [test_history_log_df.shape[0] + 1],\n",
    "            \"Created_Date\": [datetime.date.today()],\n",
    "            \"Prediction_Date\": [test_prediction_date],\n",
    "            \"RMSLE\": [rmsle],\n",
    "            \"HyperParams\": [hyper_params],\n",
    "            \"CategoricalFeatures\": [categorical_features],\n",
    "            \"LabelFeatures\": [label_features],\n",
    "            \"NSteps\": [n_steps],\n",
    "            \"Shift_list\": [shift_list],\n",
    "            \"MWM_list\": [mwm_list],\n",
    "            \"Phase\": [1],\n",
    "            \"DaysBackToConsider\": [days_back_to_consider],\n",
    "            \"SingleProblemApproach\": [single_problem_approach],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    test_history_log_df = pd.concat([test_history_log_df, current_input_df], axis=0)\n",
    "    test_history_log_df = test_history_log_df.set_index(\"Version\")\n",
    "    save_output(test_history_log_df, \"test_history_log.csv\")\n",
    "\n",
    "\n",
    "def display_test_predictions(\n",
    "    y_output: pd.DataFrame,\n",
    "    categorical_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    label_features: List[str],\n",
    "    days_back_to_display: int,\n",
    "    columns_n: int,\n",
    ") -> Figure:\n",
    "    # --- Get the correct data to display ---\n",
    "    stocks = load_data()\n",
    "    clean = clean_data(stocks)\n",
    "    features = engineer_features(\n",
    "        clean, categorical_features, shift_list=shift_list, mwm_list=mwm_list, label_features=label_features\n",
    "    )\n",
    "\n",
    "    unique_names = y_output.index.get_level_values(\"Name\").unique()\n",
    "    unique_names_n = len(unique_names)\n",
    "\n",
    "    rows_n = -(-unique_names_n // columns_n)\n",
    "\n",
    "    fig, axs = plt.subplots(rows_n, columns_n, figsize=(15, 6 * rows_n))\n",
    "\n",
    "    for i, name in enumerate(unique_names):\n",
    "\n",
    "        axs[i // columns_n, i % columns_n].set_title(f\"Close for {name}\")\n",
    "\n",
    "        cast(pd.DataFrame, features[features.index.get_level_values(\"Name\") == name]).reset_index(\"Name\")[\"Close\"].tail(\n",
    "            days_back_to_display\n",
    "        ).plot(label=f\"Close for {name}\", ax=axs[i // columns_n, i % columns_n])\n",
    "\n",
    "        cast(pd.DataFrame, y_output[y_output.index.get_level_values(\"Name\") == name]).reset_index(\"Name\")[\"Close\"].plot(\n",
    "            label=f\"Predicted Close for {name}\", ax=axs[i // columns_n, i % columns_n]\n",
    "        )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def execute_real_run(\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict = {},\n",
    "    single_problem_approach: bool = False,\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "\n",
    "    stocks = load_data()\n",
    "    clean = clean_data(stocks)\n",
    "    features = engineer_features(\n",
    "        clean, categorical_features, shift_list=shift_list, mwm_list=mwm_list, label_features=label_features\n",
    "    )\n",
    "    X, y = process_data(features, n_steps, categorical_features, days_back_to_consider=days_back_to_consider)\n",
    "    X_train, y_train = X.align(y.dropna(), axis=0, join=\"inner\")\n",
    "    X_query = X.copy(deep=True)\n",
    "\n",
    "    y_pred, run_params = solve(\n",
    "        hyper_params=hyper_params,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        X_query=X_query,\n",
    "        single_problem_approach=single_problem_approach,\n",
    "    )\n",
    "\n",
    "    y_output = convert_last_prediction_to_output(y_pred)\n",
    "\n",
    "    return y_output, run_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST EXECUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(\n",
    "    name: str,\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict,\n",
    "    single_problem_approach: bool,\n",
    "    n_optimize_trials: int,\n",
    "    save_image: bool,\n",
    "    combined: bool,\n",
    ") -> Tuple[float, pd.DataFrame, Dict]:\n",
    "\n",
    "    rmsle, y_output, best_hyper_params = execute_test_run(\n",
    "        n_steps=n_steps,\n",
    "        days_back_to_consider=days_back_to_consider,\n",
    "        categorical_features=categorical_features,\n",
    "        label_features=label_features,\n",
    "        shift_list=shift_list,\n",
    "        mwm_list=mwm_list,\n",
    "        hyper_params=hyper_params,\n",
    "        n_optimize_trials=n_optimize_trials,\n",
    "        single_problem_approach=single_problem_approach,\n",
    "        combined=combined,\n",
    "    )\n",
    "\n",
    "    save_test_logs(\n",
    "        rmsle=rmsle,\n",
    "        y_output=y_output,\n",
    "        hyper_params=best_hyper_params,\n",
    "        single_problem_approach=single_problem_approach,\n",
    "        n_steps=n_steps,\n",
    "        days_back_to_consider=days_back_to_consider,\n",
    "        categorical_features=categorical_features,\n",
    "        label_features=label_features,\n",
    "        shift_list=shift_list,\n",
    "        mwm_list=mwm_list,\n",
    "    )\n",
    "\n",
    "    fig = display_test_predictions(\n",
    "        y_output=y_output,\n",
    "        categorical_features=categorical_features,\n",
    "        shift_list=shift_list,\n",
    "        mwm_list=mwm_list,\n",
    "        label_features=label_features,\n",
    "        days_back_to_display=125,\n",
    "        columns_n=3,\n",
    "    )\n",
    "\n",
    "\n",
    "    test_prediction_date = get_nth_previous_working_date(\n",
    "        n=1, date=cast(pd.Period, y_output.index.get_level_values(\"Date\").min()).to_timestamp().date()\n",
    "    )\n",
    "    if save_image is True:\n",
    "        fig.savefig(f\"{OUTPUT_DIR_PATH}/test_prediction_{name}_{test_prediction_date}.png\")\n",
    "\n",
    "    return rmsle, y_output, best_hyper_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GENERATE PREDICTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(\n",
    "    name: str,\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict,\n",
    "    single_problem_approach: bool,\n",
    ") -> datetime.date:\n",
    "    y_real_output, run_params = execute_real_run(\n",
    "        n_steps=n_steps,\n",
    "        days_back_to_consider=days_back_to_consider,\n",
    "        categorical_features=categorical_features,\n",
    "        label_features=label_features,\n",
    "        shift_list=shift_list,\n",
    "        mwm_list=mwm_list,\n",
    "        hyper_params=hyper_params,\n",
    "        single_problem_approach=single_problem_approach,\n",
    "    )\n",
    "\n",
    "    real_prediction_date = get_nth_previous_working_date(\n",
    "        n=1, date=cast(pd.Period, y_real_output.index.get_level_values(\"Date\").min()).to_timestamp().date()\n",
    "    )\n",
    "\n",
    "    generation_df = pd.DataFrame(\n",
    "        {   \n",
    "            \"PredictionDate\": [real_prediction_date.isoformat()],\n",
    "            \"CreatedTimestamp\": [datetime.datetime.now(datetime.UTC)],\n",
    "            \"CategoricalFeatures\": [json.dumps(categorical_features)],\n",
    "            \"LabelFeatures\": [json.dumps(label_features)],\n",
    "            \"ShiftList\": [json.dumps(shift_list)],\n",
    "            \"MWMList\": [json.dumps(mwm_list)],\n",
    "            \"DaysBackToConsider\": [days_back_to_consider],\n",
    "            \"NSteps\": [n_steps],\n",
    "            \"Name\": [name],\n",
    "            \"HyperParams\": [json.dumps(hyper_params)],\n",
    "        }\n",
    "    ).set_index([\"PredictionDate\", \"Name\"])\n",
    "\n",
    "    save_output(y_real_output, f\"prediction_{real_prediction_date.isoformat()}.csv\")\n",
    "    save_output(generation_df, f\"generation_{real_prediction_date.isoformat()}.csv\")\n",
    "\n",
    "    return real_prediction_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prediction(\n",
    "    name=\"default_multi_approach\",\n",
    "    n_steps=N_STEPS,\n",
    "    days_back_to_consider=DAYS_BACK_TO_CONSIDER,\n",
    "    categorical_features=CATEGORICAL_FEATURES,\n",
    "    label_features=LABEL_FEATURES,\n",
    "    shift_list=SHIFT_LIST,\n",
    "    mwm_list=MWM_LIST,\n",
    "    hyper_params=HYPER_PARAMS,\n",
    "    single_problem_approach=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsp-NqpUKMXU-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
