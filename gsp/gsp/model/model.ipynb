{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, cast, Any, Literal, Optional\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import logging \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import RegressorChain, MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from data import SCRAPED_STOCK_FILE_PATH \n",
    "from gsp.model.utils import (\n",
    "    MOVING_WINDOW_AGGREGATORS_ALIAS, \n",
    "    make_shift_in_groups, \n",
    "    make_mw_in_groups, \n",
    "    get_most_recent_working_date, \n",
    "    get_nth_previous_working_date,\n",
    "    show,\n",
    "    get_all_missing_stock_names,\n",
    "    get_minimal_stocks_existence_date,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "stock_id = \"NVDA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stocks = (\n",
    "    pd.read_csv(\n",
    "        SCRAPED_STOCK_FILE_PATH,\n",
    "        dtype={\n",
    "            \"Date\": \"period[D]\",\n",
    "            \"Open\": \"float\",\n",
    "            \"High\": \"float\",\n",
    "            \"Low\": \"float\",\n",
    "            \"Close\": \"float\",\n",
    "            \"Volume\": \"int\",\n",
    "            \"Area\": \"category\",\n",
    "            \"Name\": \"category\",\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLOBAL PARAMETERS - SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The furthest date to consider is 2021-05-04'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Data starts from 2021-05-04 and ends at 2024-05-03.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Training data starts from 2021-05-04 and ends at 2023-12-29.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Testing data starts from 2024-02-09 and ends at 2024-03-22.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- MAIN SETUP ---\n",
    "MAIN_INDEX = [\"Date\", \"Name\"]\n",
    "SECONDARY_INDEX = [\"Name\"]\n",
    "\n",
    "YEARS_BACK_TO_CONSIDER = 3\n",
    "\"\"\"Number of years back that we want to consider in predictions.\"\"\"\n",
    "N_STEP_PREDICTION = 30\n",
    "\"\"\"Defining a forecastig task.\"\"\"\n",
    "DAYS_BACK_TO_TEST = 3*N_STEP_PREDICTION\n",
    "TODAY = cast(pd.Period, _stocks[\"Date\"].max()).to_timestamp().date()\n",
    "DATA_STARTING_DATE = TODAY - datetime.timedelta(days=365*YEARS_BACK_TO_CONSIDER)\n",
    "\"\"\"A date indicating from when at ealriest we want to consider stocks data.\"\"\"\n",
    "DATA_EXISTENCE_DATE = get_minimal_stocks_existence_date(_stocks[_stocks[\"Date\"] >= DATA_STARTING_DATE.isoformat()])\n",
    "\"\"\"A date indicating the earliest date for which we have data for all stocks.\"\"\"\n",
    "\n",
    "\n",
    "# --- TRAINING & TESTING SETUP ---\n",
    "TRAINING_END_DATE = get_nth_previous_working_date(n=DAYS_BACK_TO_TEST, date=TODAY)\n",
    "TESTING_START_DATE = get_nth_previous_working_date(n=-N_STEP_PREDICTION, date=TRAINING_END_DATE)\n",
    "TESTING_END_DATE = get_nth_previous_working_date(n=N_STEP_PREDICTION, date=TODAY)\n",
    "\n",
    "show(\n",
    "    f\"The furthest date to consider is {DATA_STARTING_DATE}\",\n",
    "    f\"Data starts from {DATA_EXISTENCE_DATE} and ends at {TODAY}.\",\n",
    "    f\"Training data starts from {DATA_EXISTENCE_DATE} and ends at {TRAINING_END_DATE}.\",\n",
    "    f\"Testing data starts from {TESTING_START_DATE} and ends at {TESTING_END_DATE}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_stocks(stocks: pd.DataFrame, main_index:List[str], start_date: datetime.date, existence_date: datetime.date) -> pd.DataFrame:\n",
    "    missing_stocks_data = get_all_missing_stock_names(_stocks, start_date)\n",
    "    logger.info(f\"Missing stocks data: {missing_stocks_data}\")\n",
    "    logger.info(f\"Data existence date: {existence_date}\")\n",
    "    return stocks.copy().set_index(main_index).sort_index().unstack(\"Name\").ffill().stack(\"Name\", future_stack=True).loc[existence_date.isoformat() :]  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessX(\n",
    "    data: pd.DataFrame,\n",
    "    secondary_index: List[str],\n",
    "    shifts_list: List[int],\n",
    "    windows_list: List[int],\n",
    "    aggregators_list: List[MOVING_WINDOW_AGGREGATORS_ALIAS] | MOVING_WINDOW_AGGREGATORS_ALIAS,\n",
    ") -> pd.DataFrame:\n",
    "    data = data.copy()\n",
    "\n",
    "    data[\"DayOfWeek\"] = data.index.get_level_values(\"Date\").dayofweek # type: ignore\n",
    "    data[\"Month\"] = data.index.get_level_values(\"Date\").month # type: ignore\n",
    "    data[\"Year\"] = data.index.get_level_values(\"Date\").year # type: ignore\n",
    "    data[\"IsMonthStart\"] = data.index.get_level_values(\"Date\").day < 5 # type: ignore\n",
    "    data[\"IsMonthEnd\"] = data.index.get_level_values(\"Date\").days_in_month - data.index.get_level_values(\"Date\").day < 5 # type: ignore\n",
    "    data[\"Quarter\"] = data.index.get_level_values(\"Date\").quarter # type: ignore\n",
    "    data[\"NameCat\"] = data.index.get_level_values(\"Name\")\n",
    "    # Prodcut lunches from those comapnies\n",
    "\n",
    "    grouped_lags: List[pd.DataFrame | pd.Series] = [\n",
    "        make_shift_in_groups(df=data, groupby=secondary_index, column=\"Close\", shift=shifts_list),\n",
    "        make_mw_in_groups(df=data, groupby=secondary_index, column=\"Close\", window=windows_list, aggregator=aggregators_list),\n",
    "    ]\n",
    "\n",
    "    data_lag = data.join(grouped_lags)\n",
    "\n",
    "    categorical_features = [\"Area\",\"NameCat\",\"DayOfWeek\", \"Month\", \"Year\",\"IsMonthStart\", \"IsMonthEnd\", \"Quarter\"]\n",
    "\n",
    "    data_encoded = pd.get_dummies(data_lag, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    return data_encoded.drop(columns=[\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"])\n",
    "\n",
    "\n",
    "def preprocessY(data: pd.DataFrame, secondary_index: List[str], n_step_prediction:int) -> pd.DataFrame:\n",
    "    data = data.copy()\n",
    "    return make_shift_in_groups(df = data, groupby=secondary_index, column=\"Close\", shift=[-i for i in range(1,n_step_prediction+1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X: pd.DataFrame, y: pd.DataFrame, training_end_date: datetime.date, testing_start_date: datetime.date, testing_end_date: datetime.date) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    X_train, y_train = (X.loc[: training_end_date.isoformat()], y.loc[: training_end_date.isoformat()])\n",
    "    X_test, y_test = (\n",
    "        X.loc[testing_start_date.isoformat() : testing_end_date.isoformat()],\n",
    "        y.loc[testing_start_date.isoformat() : testing_end_date.isoformat()],\n",
    "    )\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(**hyper_params):\n",
    "    # model = LinearRegression(**hyper_params)\n",
    "    model = RegressorChain(XGBRegressor(**hyper_params))\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAININING & TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(model, X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"Fitting the model...\")\n",
    "    model = model.fit(X_train, y_train)\n",
    "    logger.info(\"Predicting...\")\n",
    "    y_pred = pd.DataFrame(data=model.predict(X_test), index=X_test.index, columns=y_train.columns)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multistep(y, every=1, ax=None, palette_kwargs=None):\n",
    "    palette_kwargs_ = dict(palette=\"husl\", n_colors=16, desat=None)\n",
    "    if palette_kwargs is not None:\n",
    "        palette_kwargs_.update(palette_kwargs)\n",
    "    palette = sns.color_palette(**palette_kwargs_)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.set_prop_cycle(plt.cycler(\"color\", palette))\n",
    "    for date, preds in y[::every].iterrows():\n",
    "        preds.index = pd.period_range(start=date, periods=len(preds))\n",
    "        preds.plot(ax=ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO __main__ 18:17:26 | 2024-03-22 is the date of the error, and 2024-05-03 is the leading date.\n",
      "INFO __main__ 18:17:27 | Missing stocks data: []\n",
      "INFO __main__ 18:17:27 | Data existence date: 2021-05-04\n",
      "INFO __main__ 18:19:19 | Demo done\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ColumnTransformerWrapper:\n",
    "    transformers : List[Tuple[str, Any, List[str]]]\n",
    "    remainder : Literal[\"drop\", \"passthrough\"] = \"passthrough\"\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y: Optional[Any] = None) -> pd.DataFrame: # type: ignore\n",
    "        ct = ColumnTransformer(self.transformers, remainder=self.remainder)\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            ct.fit_transform(X, y), # type: ignore\n",
    "            index=X.index,\n",
    "            columns=[col.replace(\"remainder__\",\"\") for col in ct.get_feature_names_out()],\n",
    "        )\n",
    "\n",
    "\n",
    "def demo(stocks: pd.DataFrame,\n",
    "    model,\n",
    "    data_starting_date : datetime.date = DATA_STARTING_DATE,\n",
    "    data_existence_date: datetime.date = DATA_EXISTENCE_DATE,\n",
    "    data_training_end_date: datetime.date = TRAINING_END_DATE,\n",
    "    data_testing_start_date: datetime.date = TESTING_START_DATE,\n",
    "    data_testing_end_date: datetime.date = TESTING_END_DATE,   \n",
    "    main_index : List[str]= MAIN_INDEX, \n",
    "    secondary_index : List[str] = SECONDARY_INDEX,\n",
    "    n_step_prediction: int = N_STEP_PREDICTION,\n",
    "    evaluate_single_problem: bool = False) -> None:\n",
    "\n",
    "    error_date = datetime.date(year=2024, month=3, day=22)\n",
    "    leading_date = get_nth_previous_working_date(n=-30, date=error_date)\n",
    "\n",
    "    logger.info(f\"{error_date} is the date of the error, and {leading_date} is the leading date.\")\n",
    "\n",
    "    cln = clean_up_stocks(stocks[stocks[\"Name\"]==stock_id], main_index, data_starting_date, data_existence_date)\n",
    "\n",
    "    # data = cln.copy()\n",
    "    # data[\"DayOfWeek\"] = data.index.get_level_values(\"Date\").dayofweek  # type: ignore\n",
    "    # data[\"Month\"] = data.index.get_level_values(\"Date\").month  # type: ignore\n",
    "    # data[\"Year\"] = data.index.get_level_values(\"Date\").year  # type: ignore\n",
    "    # data[\"IsMonthStart\"] = data.index.get_level_values(\"Date\").day < 5  # type: ignore\n",
    "    # data[\"IsMonthEnd\"] = data.index.get_level_values(\"Date\").days_in_month - data.index.get_level_values(\"Date\").day < 5  # type: ignore\n",
    "    # data[\"Quarter\"] = data.index.get_level_values(\"Date\").quarter  # type: ignore\n",
    "    # data[\"NameCat\"] = data.index.get_level_values(\"Name\")\n",
    "    # categorical_features = [\"Area\", \"NameCat\", \"DayOfWeek\", \"Month\", \"Year\", \"IsMonthStart\", \"IsMonthEnd\", \"Quarter\"]\n",
    "    # ctw = ColumnTransformerWrapper(\n",
    "    #     transformers=[\n",
    "    #         (\"onehot\", OneHotEncoder(drop=\"first\"), categorical_features),\n",
    "    #     ]\n",
    "    # )\n",
    "    # encoded_hot = ctw.fit_transform(data)\n",
    "    # encoded_final = encoded_hot.drop(columns=[\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"])\n",
    "\n",
    "    y = make_shift_in_groups(\n",
    "        df=cln, groupby=secondary_index, column=\"Close\", shift=[-30]\n",
    "    )\n",
    "    y[\"dow\"] = y.index.get_level_values(\"Date\").dayofweek\n",
    "\n",
    "    logger.info(\"Demo done\")\n",
    "\n",
    "demo(\n",
    "    stocks=_stocks,\n",
    "    model=create_model(),\n",
    "    data_starting_date=DATA_STARTING_DATE,\n",
    "    data_existence_date=DATA_EXISTENCE_DATE,\n",
    "    data_training_end_date=TRAINING_END_DATE,\n",
    "    data_testing_start_date=TESTING_START_DATE,\n",
    "    data_testing_end_date=TESTING_END_DATE,\n",
    "    main_index=MAIN_INDEX,\n",
    "    secondary_index=SECONDARY_INDEX,\n",
    "    n_step_prediction=N_STEP_PREDICTION,\n",
    "    evaluate_single_problem=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retest_pipeline(\n",
    "    stocks: pd.DataFrame,\n",
    "    model,\n",
    "    data_starting_date : datetime.date = DATA_STARTING_DATE,\n",
    "    data_existence_date: datetime.date = DATA_EXISTENCE_DATE,\n",
    "    data_training_end_date: datetime.date = TRAINING_END_DATE,\n",
    "    data_testing_start_date: datetime.date = TESTING_START_DATE,\n",
    "    data_testing_end_date: datetime.date = TESTING_END_DATE,   \n",
    "    main_index : List[str]= MAIN_INDEX, \n",
    "    secondary_index : List[str] = SECONDARY_INDEX,\n",
    "    n_step_prediction: int = N_STEP_PREDICTION,\n",
    "    evaluate_single_problem: bool = False\n",
    ") -> Tuple[float, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    # --- CLEAN ---\n",
    "    stocks_cleaned = clean_up_stocks(stocks, main_index=main_index, start_date=data_starting_date, existence_date=data_existence_date)\n",
    "\n",
    "    # --- PREPROCESS ---\n",
    "    X_processed = preprocessX(\n",
    "        stocks_cleaned, secondary_index=secondary_index, shifts_list=[1,2,3], windows_list=[4, 8, 16, 24], aggregators_list=\"mean\"\n",
    "    )\n",
    "    y_processed = preprocessY(stocks_cleaned, secondary_index=secondary_index, n_step_prediction=n_step_prediction)\n",
    "\n",
    "    # --- ALIGN ---\n",
    "    y, X = y_processed.align(X_processed.dropna(), join=\"inner\", axis=0)\n",
    "\n",
    "    # --- SPLIT ---\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, training_end_date=data_training_end_date, testing_start_date=data_testing_start_date, testing_end_date=data_testing_end_date)\n",
    "\n",
    "    # --- TRAIN & TEST ---\n",
    "    y_pred = pd.DataFrame()\n",
    "    if not evaluate_single_problem:\n",
    "        y_pred = fit_predict(model, X_train, y_train, X_test)\n",
    "        error = cast(float, mean_squared_error(y_test, y_pred))\n",
    "    else:\n",
    "        error = 0.0\n",
    "        unique_names = X.index.get_level_values(\"Name\").unique()\n",
    "        for name in unique_names:\n",
    "            X_train_single = X_train.loc[X_train.index.get_level_values(\"Name\") == name]\n",
    "            y_train_single = y_train.loc[y_train.index.get_level_values(\"Name\") == name]\n",
    "            X_test_single = X_test.loc[X_test.index.get_level_values(\"Name\") == name]\n",
    "            y_test_single = y_test.loc[y_test.index.get_level_values(\"Name\") == name]\n",
    "\n",
    "            y_pred_single = fit_predict(model, X_train_single, y_train_single, X_test_single)\n",
    "            y_pred = pd.concat([y_pred, y_pred_single], axis=0)\n",
    "            error += cast(float, mean_squared_error(y_test_single, y_pred_single))\n",
    "\n",
    "    return error, y_test, y_pred, X, y\n",
    "\n",
    "error, y_test, y_pred, X, y = retest_pipeline(\n",
    "    stocks=_stocks,\n",
    "    model=create_model(),\n",
    "    data_starting_date=DATA_STARTING_DATE,\n",
    "    data_existence_date=DATA_EXISTENCE_DATE,\n",
    "    data_training_end_date=TRAINING_END_DATE,\n",
    "    data_testing_start_date=TESTING_START_DATE,\n",
    "    data_testing_end_date=TESTING_END_DATE,\n",
    "    main_index=MAIN_INDEX,\n",
    "    secondary_index=SECONDARY_INDEX,\n",
    "    n_step_prediction=N_STEP_PREDICTION,\n",
    "    evaluate_single_problem=False,\n",
    ")\n",
    "\n",
    "# show(error, y_test, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "y.loc[y.index.get_level_values(\"Name\") == stock_id][\"Close_lead_1\"].reset_index(\"Name\", drop=True).tail(300).plot(\n",
    "    ax=fig.gca()\n",
    ")\n",
    "plot_multistep(\n",
    "    y_pred.loc[y_pred.index.get_level_values(\"Name\") == stock_id].reset_index(\"Name\", drop=True), every=8, ax=fig.gca()\n",
    ")\n",
    "\n",
    "\n",
    "# 705097.7046092672\n",
    "# 602875.5865461574\n",
    "# 336965.0387972004"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsp-xcVphxGa-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
