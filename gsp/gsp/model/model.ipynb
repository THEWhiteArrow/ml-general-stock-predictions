{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, cast, Dict\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from gsp.utils.column_transformer_wrapper import ColumnTransformerWrapper\n",
    "from gsp.utils.group_shifts import make_mw_in_groups, make_shift_in_groups\n",
    "from gsp.utils.date_utils import get_nth_previous_working_date\n",
    "from data import SCRAPED_STOCK_FILE_PATH, OUTPUT_DIR_PATH, save_output, load_output\n",
    "from lib.logger.setup import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "N_STEPS: int = 21\n",
    "N_OPTIMIZE_TRIALS: int = -1  # --- NOTICE --- Negative value means no optimization\n",
    "DAYS_BACK_TO_CONSIDER: int = 5 * 252\n",
    "LABEL_FEATURES: List[str] = [\"year\"]\n",
    "CATEGORICAL_FEATURES: List[str] = [\"day_of_week\", \"area_cat\"]\n",
    "SHIFT_LIST: List[int] = [1, 2, 3]\n",
    "MWM_LIST: List[int] = [5, 10, 15]\n",
    "HYPER_PARAMS: Dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    return cast(float, mean_squared_log_error(y_true, y_pred) ** 0.5)\n",
    "\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        SCRAPED_STOCK_FILE_PATH,\n",
    "        dtype={\n",
    "            \"date\": \"period[D]\",\n",
    "            \"open\": \"float\",\n",
    "            \"high\": \"float\",\n",
    "            \"low\": \"float\",\n",
    "            \"close\": \"float\",\n",
    "            \"volume\": \"int\",\n",
    "            \"area\": \"category\",\n",
    "            \"symbol\": \"category\",\n",
    "        },\n",
    "    ).drop(columns=[\"company\"])\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame, run_date: datetime.date) -> pd.DataFrame:\n",
    "    df = df.copy(deep=True)\n",
    "    periods = pd.date_range(start=df[\"date\"].min().to_timestamp().date(), end=run_date, freq=\"B\")\n",
    "    periods_df = pd.DataFrame({\"date\": periods}, dtype=\"period[D]\")\n",
    "\n",
    "    clean_data = (\n",
    "        cast(\n",
    "            pd.DataFrame,\n",
    "            periods_df.set_index(\"date\")\n",
    "            .join(df.set_index(\"date\"))\n",
    "            .set_index(\"symbol\", append=True)\n",
    "            .sort_index()\n",
    "            .unstack(\"symbol\")\n",
    "            .ffill()\n",
    "            .bfill()  # --- NOTICE --- This is a fix so that the single problem approach works. It is not the best way to handle missing data.\n",
    "            .stack(\"symbol\", future_stack=True),  # type: ignore\n",
    "        )\n",
    "        .reset_index()\n",
    "        .dropna(subset=[\"symbol\"])\n",
    "        .set_index([\"date\", \"symbol\"])\n",
    "    )\n",
    "\n",
    "    return clean_data\n",
    "\n",
    "\n",
    "def engineer_features(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_features: List[str] = [],\n",
    "    shift_list: List[int] = [],\n",
    "    mwm_list: List[int] = [],\n",
    "    label_features: List[str] = [],\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    cat_features_to_use: List[str] = [*categorical_features, *label_features]\n",
    "\n",
    "    if \"day_of_week\" in cat_features_to_use:\n",
    "        df[\"day_of_week\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"date\")).dayofweek\n",
    "\n",
    "    if \"month\" in cat_features_to_use:\n",
    "        df[\"month\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"date\")).month\n",
    "\n",
    "    if \"year\" in cat_features_to_use:\n",
    "        df[\"year\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"date\")).year\n",
    "\n",
    "    if \"week_of_year\" in cat_features_to_use:\n",
    "        df[\"week_of_year\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"date\")).week  # type: ignore\n",
    "\n",
    "    if \"day_of_month\" in cat_features_to_use:\n",
    "        df[\"day_of_month\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"date\")).day\n",
    "\n",
    "    if \"quarter\" in cat_features_to_use:\n",
    "        df[\"quarter\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"date\")).quarter\n",
    "\n",
    "    if \"area_cat\" in cat_features_to_use:\n",
    "        df[\"area_cat\"] = df[\"area\"]\n",
    "\n",
    "    grouped_lags: List[pd.DataFrame | pd.Series] = [\n",
    "        make_shift_in_groups(df, groupby=[\"symbol\"], column=\"close\", shift=shift_list),\n",
    "        make_mw_in_groups(df, groupby=[\"symbol\"], column=\"close\", window=mwm_list),\n",
    "    ]\n",
    "\n",
    "    df_grouped_lags = df.join(grouped_lags, how=\"left\")\n",
    "\n",
    "    return df_grouped_lags\n",
    "\n",
    "\n",
    "def process_data(\n",
    "    df: pd.DataFrame,\n",
    "    n_steps: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str] = [],\n",
    "    days_back_to_consider: int | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    \"\"\"First date where all stocks have data\"\"\"\n",
    "    first_all_valid_date: datetime.date = (\n",
    "        cast(pd.Period, df.unstack(\"symbol\").dropna().first_valid_index()).to_timestamp().date()\n",
    "    )\n",
    "\n",
    "    latest_date: datetime.date = cast(pd.Period, df.index.get_level_values(\"date\").max()).to_timestamp().date()\n",
    "\n",
    "    earliest_date: datetime.date = first_all_valid_date\n",
    "\n",
    "    starting_date_to_consider: datetime.date | None = None\n",
    "\n",
    "    if days_back_to_consider is not None:\n",
    "        starting_date_to_consider = get_nth_previous_working_date(n=days_back_to_consider, date=latest_date)\n",
    "\n",
    "    if starting_date_to_consider is not None and starting_date_to_consider < first_all_valid_date:\n",
    "        logger.warning(\n",
    "            f\"WARNING: starting_date_to_consider ({starting_date_to_consider}) is before the first date where all stocks have data. Using {first_all_valid_date} instead.\"\n",
    "        )\n",
    "    elif starting_date_to_consider is not None:\n",
    "        earliest_date = starting_date_to_consider\n",
    "\n",
    "    logger.info(f\"Earliest date: {earliest_date.isoformat()}\")\n",
    "\n",
    "    ctw = ColumnTransformerWrapper(\n",
    "        transformers=[\n",
    "            (\"onehot\", OneHotEncoder(drop=\"first\"), categorical_features),\n",
    "            (\"label\", LabelEncoder(), label_features),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    df_from_earliest = df.loc[earliest_date.isoformat() :]  # type: ignore\n",
    "    X = ctw.fit_transform(df_from_earliest.drop(columns=[\"adj_close\", \"volume\", \"high\", \"low\", \"open\", \"area\"]))\n",
    "    y = make_shift_in_groups(df, groupby=[\"symbol\"], column=\"close\", shift=[-i for i in range(1, n_steps + 1)])\n",
    "\n",
    "    y, X = y.align(X.dropna(), axis=0, join=\"inner\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    n_steps: int,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"A function that splits the time series data into train and test sets in regards\n",
    "    to the number of forecasting steps. It will test the model on the last 2*n_steps.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Features dataframe\n",
    "        y (pd.DataFrame): Target dataframe\n",
    "        n_steps (int): Number of steps of the forecasting task\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    latest_date = cast(pd.Period, X.index.get_level_values(\"date\").max()).to_timestamp().date()\n",
    "    test_end_date = get_nth_previous_working_date(n=n_steps, date=latest_date)\n",
    "    test_start_date = get_nth_previous_working_date(n=2 * n_steps - 1, date=latest_date)\n",
    "    train_end_date = get_nth_previous_working_date(n=2 * n_steps, date=latest_date)\n",
    "\n",
    "    X_train = X.loc[: train_end_date.isoformat()]  # type: ignore\n",
    "    y_train = y.loc[: train_end_date.isoformat()]  # type: ignore\n",
    "    X_test = X.loc[test_start_date.isoformat() : test_end_date.isoformat()]  # type: ignore\n",
    "    y_test = y.loc[test_start_date.isoformat() : test_end_date.isoformat()]  # type: ignore\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def create_model(**hyper_params):\n",
    "    model = RegressorChain(XGBRegressor(**hyper_params))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_last_prediction_to_output(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    latest_date = cast(pd.Period, df.index.get_level_values(\"date\").max()).to_timestamp().date()\n",
    "\n",
    "    y_latest = cast(pd.DataFrame, df.loc[latest_date.isoformat()]).rename(\n",
    "        columns={\n",
    "            f\"close_lead_{i}\": get_nth_previous_working_date(n=-i, date=latest_date).isoformat()\n",
    "            for i in range(1, len(df.columns) + 1)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    y_output = (\n",
    "        y_latest.reset_index()\n",
    "        .melt(id_vars=[\"symbol\"], var_name=\"date\", value_name=\"close\")\n",
    "        .astype(\n",
    "            {\n",
    "                \"date\": \"period[D]\",\n",
    "                \"symbol\": \"category\",\n",
    "                \"close\": \"float\",\n",
    "            }\n",
    "        )\n",
    "        .set_index([\"date\", \"symbol\"])\n",
    "    )\n",
    "\n",
    "    return y_output\n",
    "\n",
    "\n",
    "def solve(\n",
    "    hyper_params: Dict,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_query: pd.DataFrame,\n",
    "    single_problem_approach: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    y_pred = pd.DataFrame()\n",
    "    if single_problem_approach is False:\n",
    "        logger.info(\"Training and predicting for all stocks\")\n",
    "        model = create_model(**hyper_params)\n",
    "        model.fit(X, y)\n",
    "        y_pred = pd.DataFrame(model.predict(X_query), index=X_query.index, columns=y.columns)  # type: ignore\n",
    "    else:\n",
    "        unique_stock_names = X.index.get_level_values(\"symbol\").unique()\n",
    "        for i, name in enumerate(unique_stock_names):\n",
    "            logger.info(f\"Training and predicting for {name} | {i + 1}/{len(unique_stock_names)}\")\n",
    "            X_single = cast(pd.DataFrame, X.loc[X.index.get_level_values(\"symbol\") == name])\n",
    "            y_single = cast(pd.DataFrame, y.loc[y.index.get_level_values(\"symbol\") == name])\n",
    "            X_query_single = cast(pd.DataFrame, X_query.loc[X_query.index.get_level_values(\"symbol\") == name])\n",
    "            model = create_model(**hyper_params)\n",
    "            model.fit(X_single, y_single)\n",
    "            y_pred_single = pd.DataFrame(model.predict(X_query_single), index=X_query_single.index, columns=y_single.columns)  # type: ignore\n",
    "            y_pred = pd.concat([y_pred, y_pred_single], axis=0)\n",
    "\n",
    "    y_pred = y_pred.sort_index().clip(lower=0)\n",
    "    logger.info(\"Prediction done\")\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def optimize_with_optuna(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "    single_problem_approach: bool,\n",
    "    n_optimize_trials: int,\n",
    ") -> Tuple[float, Dict]:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=f\"optuna_optimization_{'single_approach' if single_problem_approach else \"multi_approach\"}\",\n",
    "    )\n",
    "\n",
    "    def optuna_optimize_objective(trial: optuna.Trial) -> float:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1200, step=50),  # Number of trees in the ensemble\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),  # Maximum depth of each tree\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),  # Learning rate\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),  # Subsample ratio of the training instances\n",
    "            \"colsample_bytree\": trial.suggest_float(\n",
    "                \"colsample_bytree\", 0.5, 1.0\n",
    "            ),  # Subsample ratio of columns when constructing each tree\n",
    "            \"gamma\": trial.suggest_float(\n",
    "                \"gamma\", 0.01, 10.0, log=True\n",
    "            ),  # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 100.0, log=True),  # L1 regularization term on weights\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 100.0, log=True),  # L2 regularization term on weights\n",
    "            \"min_child_weight\": trial.suggest_float(\n",
    "                \"min_child_weight\", 1, 100, log=True\n",
    "            ),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "        }\n",
    "\n",
    "        y_pred = solve(\n",
    "            hyper_params=params,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            X_query=X_test,\n",
    "            single_problem_approach=single_problem_approach,\n",
    "        )\n",
    "        rmsle = root_mean_squared_log_error(y_test, y_pred)\n",
    "        return rmsle\n",
    "\n",
    "    study.optimize(optuna_optimize_objective, n_trials=n_optimize_trials)\n",
    "    best_params = study.best_params\n",
    "    best_rmsle = study.best_value\n",
    "\n",
    "    return best_rmsle, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_test_run(\n",
    "    run_date: datetime.date,\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict = {},\n",
    "    n_optimize_trials: int = -1,\n",
    "    single_problem_approach: bool = False,\n",
    "    combined: bool = False,\n",
    "):\n",
    "\n",
    "    stocks = load_data()\n",
    "    clean = clean_data(stocks, run_date)\n",
    "    features = engineer_features(\n",
    "        clean, categorical_features, shift_list=shift_list, mwm_list=mwm_list, label_features=label_features\n",
    "    )\n",
    "    X, y = process_data(features, n_steps, categorical_features, days_back_to_consider=days_back_to_consider)\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, n_steps)\n",
    "\n",
    "    # --- Setup for the test run ---\n",
    "    final_rmsle: float = 0.0\n",
    "    final_y_pred: pd.DataFrame = pd.DataFrame()\n",
    "    final_hyper_params: Dict = {}\n",
    "\n",
    "    # --- Training and predicting ---\n",
    "    if combined is False:\n",
    "        y_default_pred = solve(\n",
    "            hyper_params=hyper_params,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            X_query=X_test,\n",
    "            single_problem_approach=single_problem_approach,\n",
    "        )\n",
    "    else:\n",
    "        y_single_pred = solve(\n",
    "            hyper_params=hyper_params,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            X_query=X_test,\n",
    "            single_problem_approach=False,\n",
    "        )\n",
    "        y_multi_pred = solve(\n",
    "            hyper_params=hyper_params,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            X_query=X_test,\n",
    "            single_problem_approach=True,\n",
    "        )\n",
    "\n",
    "        y_default_pred = (y_single_pred + y_multi_pred) / 2\n",
    "\n",
    "    rmsle_default = root_mean_squared_log_error(y_test, y_default_pred)\n",
    "\n",
    "    final_rmsle = rmsle_default\n",
    "    final_y_pred = y_default_pred\n",
    "    final_hyper_params = hyper_params\n",
    "\n",
    "    if n_optimize_trials > 0:\n",
    "        optuna_rmsle, optuna_hyper_params = optimize_with_optuna(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            single_problem_approach=single_problem_approach,\n",
    "            n_optimize_trials=n_optimize_trials,\n",
    "        )\n",
    "\n",
    "        if optuna_rmsle < final_rmsle:\n",
    "\n",
    "            final_hyper_params = optuna_hyper_params\n",
    "            final_rmsle = optuna_rmsle\n",
    "            final_y_pred = solve(\n",
    "                hyper_params=optuna_hyper_params,\n",
    "                X=X_train,\n",
    "                y=y_train,\n",
    "                X_query=X_test,\n",
    "                single_problem_approach=single_problem_approach,\n",
    "            )\n",
    "\n",
    "    y_output = convert_last_prediction_to_output(final_y_pred)\n",
    "    return final_rmsle, y_output, final_hyper_params\n",
    "\n",
    "\n",
    "def save_test_logs(\n",
    "    rmsle: float,\n",
    "    y_output: pd.DataFrame,\n",
    "    hyper_params: Dict,\n",
    "    single_problem_approach: bool,\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    ") -> None:\n",
    "    test_history_log_df = load_output(\"test_history_log.csv\")\n",
    "    test_prediction_date = get_nth_previous_working_date(\n",
    "        n=1, date=cast(pd.Period, y_output.index.get_level_values(\"date\").min()).to_timestamp().date()\n",
    "    )\n",
    "    current_input_df = pd.DataFrame(\n",
    "        {\n",
    "            \"version\": [test_history_log_df.shape[0] + 1],\n",
    "            \"created_at\": [datetime.date.today()],\n",
    "            \"date\": [test_prediction_date],\n",
    "            \"rmsle\": [rmsle],\n",
    "            \"hyper_params\": [hyper_params],\n",
    "            \"categorical_features\": [categorical_features],\n",
    "            \"label_features\": [label_features],\n",
    "            \"n_step\": [n_steps],\n",
    "            \"shifts\": [shift_list],\n",
    "            \"mwms\": [mwm_list],\n",
    "            \"phase\": [1],\n",
    "            \"days_back_to_consider\": [days_back_to_consider],\n",
    "            \"single_problem_approach\": [single_problem_approach],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    test_history_log_df = pd.concat([test_history_log_df, current_input_df], axis=0)\n",
    "    test_history_log_df = test_history_log_df.set_index(\"version\")\n",
    "    save_output(test_history_log_df, \"test_history_log.csv\")\n",
    "\n",
    "\n",
    "def display_test_predictions(\n",
    "    run_date: datetime.date,\n",
    "    y_output: pd.DataFrame,\n",
    "    categorical_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    label_features: List[str],\n",
    "    days_back_to_display: int,\n",
    "    columns_n: int,\n",
    ") -> Figure:\n",
    "    # --- Get the correct data to display ---\n",
    "    stocks = load_data()\n",
    "    clean = clean_data(stocks, run_date)\n",
    "    features = engineer_features(\n",
    "        clean, categorical_features, shift_list=shift_list, mwm_list=mwm_list, label_features=label_features\n",
    "    )\n",
    "\n",
    "    unique_names = y_output.index.get_level_values(\"symbol\").unique()\n",
    "    unique_names_n = len(unique_names)\n",
    "\n",
    "    rows_n = -(-unique_names_n // columns_n)\n",
    "\n",
    "    fig, axs = plt.subplots(rows_n, columns_n, figsize=(15, 6 * rows_n))\n",
    "\n",
    "    for i, name in enumerate(unique_names):\n",
    "\n",
    "        axs[i // columns_n, i % columns_n].set_title(f\"Close for {name}\")\n",
    "\n",
    "        cast(pd.DataFrame, features[features.index.get_level_values(\"symbol\") == name]).reset_index(\"symbol\")[\n",
    "            \"close\"\n",
    "        ].tail(days_back_to_display).plot(label=f\"Close for {name}\", ax=axs[i // columns_n, i % columns_n])\n",
    "\n",
    "        cast(pd.DataFrame, y_output[y_output.index.get_level_values(\"symbol\") == name]).reset_index(\"symbol\")[\n",
    "            \"close\"\n",
    "        ].plot(label=f\"Predicted Close for {name}\", ax=axs[i // columns_n, i % columns_n])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def execute_real_run(\n",
    "    run_date: datetime.date,\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict = {},\n",
    "    single_problem_approach: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    stocks = load_data()\n",
    "    clean = clean_data(stocks, run_date=run_date)\n",
    "    features = engineer_features(\n",
    "        clean, categorical_features, shift_list=shift_list, mwm_list=mwm_list, label_features=label_features\n",
    "    )\n",
    "    X, y = process_data(features, n_steps, categorical_features, days_back_to_consider=days_back_to_consider)\n",
    "    X_train, y_train = X.align(y.dropna(), axis=0, join=\"inner\")\n",
    "    X_query = X.copy(deep=True)\n",
    "\n",
    "    y_pred = solve(\n",
    "        hyper_params=hyper_params,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        X_query=X_query,\n",
    "        single_problem_approach=single_problem_approach,\n",
    "    )\n",
    "\n",
    "    y_output = convert_last_prediction_to_output(y_pred)\n",
    "\n",
    "    return y_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST EXECUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(\n",
    "    run_date: datetime.date,\n",
    "    name: str,\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict,\n",
    "    single_problem_approach: bool,\n",
    "    n_optimize_trials: int,\n",
    "    save_image: bool,\n",
    "    combined: bool,\n",
    ") -> Tuple[float, pd.DataFrame, Dict]:\n",
    "\n",
    "    rmsle, y_output, best_hyper_params = execute_test_run(\n",
    "        run_date=run_date,\n",
    "        n_steps=n_steps,\n",
    "        days_back_to_consider=days_back_to_consider,\n",
    "        categorical_features=categorical_features,\n",
    "        label_features=label_features,\n",
    "        shift_list=shift_list,\n",
    "        mwm_list=mwm_list,\n",
    "        hyper_params=hyper_params,\n",
    "        n_optimize_trials=n_optimize_trials,\n",
    "        single_problem_approach=single_problem_approach,\n",
    "        combined=combined,\n",
    "    )\n",
    "\n",
    "    save_test_logs(\n",
    "        rmsle=rmsle,\n",
    "        y_output=y_output,\n",
    "        hyper_params=best_hyper_params,\n",
    "        single_problem_approach=single_problem_approach,\n",
    "        n_steps=n_steps,\n",
    "        days_back_to_consider=days_back_to_consider,\n",
    "        categorical_features=categorical_features,\n",
    "        label_features=label_features,\n",
    "        shift_list=shift_list,\n",
    "        mwm_list=mwm_list,\n",
    "    )\n",
    "\n",
    "    fig = display_test_predictions(\n",
    "        y_output=y_output,\n",
    "        categorical_features=categorical_features,\n",
    "        shift_list=shift_list,\n",
    "        mwm_list=mwm_list,\n",
    "        label_features=label_features,\n",
    "        days_back_to_display=125,\n",
    "        columns_n=3,\n",
    "    )\n",
    "\n",
    "    test_prediction_date = get_nth_previous_working_date(\n",
    "        n=1, date=cast(pd.Period, y_output.index.get_level_values(\"date\").min()).to_timestamp().date()\n",
    "    )\n",
    "    if save_image is True:\n",
    "        fig.savefig(f\"{OUTPUT_DIR_PATH}/test_prediction_{name}_{test_prediction_date}.png\")\n",
    "\n",
    "    return rmsle, y_output, best_hyper_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GENERATE PREDICTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(\n",
    "    run_date: datetime.date,\n",
    "    name: str,\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict,\n",
    "    single_problem_approach: bool,\n",
    "):\n",
    "    y_real_output = execute_real_run(\n",
    "        run_date=run_date,\n",
    "        n_steps=n_steps,\n",
    "        days_back_to_consider=days_back_to_consider,\n",
    "        categorical_features=categorical_features,\n",
    "        label_features=label_features,\n",
    "        shift_list=shift_list,\n",
    "        mwm_list=mwm_list,\n",
    "        hyper_params=hyper_params,\n",
    "        single_problem_approach=single_problem_approach,\n",
    "    )\n",
    "\n",
    "    generation_df = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": [run_date.isoformat()],\n",
    "            \"created_at\": [datetime.datetime.now(datetime.UTC)],\n",
    "            \"categorical_features\": [json.dumps(categorical_features)],\n",
    "            \"label_features\": [json.dumps(label_features)],\n",
    "            \"shifts\": [json.dumps(shift_list)],\n",
    "            \"mwms\": [json.dumps(mwm_list)],\n",
    "            \"days_back_to_consider\": [days_back_to_consider],\n",
    "            \"n_step\": [n_steps],\n",
    "            \"name\": [name],\n",
    "            \"hyper_params\": [json.dumps(hyper_params)],\n",
    "        }\n",
    "    ).set_index([\"date\", \"name\"])\n",
    "\n",
    "    save_output(y_real_output, f\"prediction_{run_date.isoformat()}.csv\")\n",
    "    save_output(generation_df, f\"generation_{run_date.isoformat()}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO __main__ 13:13:13 | Earliest date: 2019-03-04\n",
      "INFO __main__ 13:13:14 | Training and predicting for all stocks\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault_multi_approach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdays_back_to_consider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDAYS_BACK_TO_CONSIDER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCATEGORICAL_FEATURES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_FEATURES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshift_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSHIFT_LIST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmwm_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMWM_LIST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHYPER_PARAMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_problem_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36mgenerate_prediction\u001b[0;34m(run_date, name, n_steps, days_back_to_consider, categorical_features, label_features, shift_list, mwm_list, hyper_params, single_problem_approach)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prediction\u001b[39m(\n\u001b[1;32m      2\u001b[0m     run_date: datetime\u001b[38;5;241m.\u001b[39mdate,\n\u001b[1;32m      3\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     single_problem_approach: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m     12\u001b[0m ):\n\u001b[0;32m---> 13\u001b[0m     y_real_output \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_real_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdays_back_to_consider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdays_back_to_consider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshift_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmwm_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmwm_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_problem_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msingle_problem_approach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     generation_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     26\u001b[0m         {\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: [run_date\u001b[38;5;241m.\u001b[39misoformat()],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m         }\n\u001b[1;32m     38\u001b[0m     )\u001b[38;5;241m.\u001b[39mset_index([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     40\u001b[0m     save_output(y_real_output, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_date\u001b[38;5;241m.\u001b[39misoformat()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 187\u001b[0m, in \u001b[0;36mexecute_real_run\u001b[0;34m(run_date, n_steps, days_back_to_consider, categorical_features, label_features, shift_list, mwm_list, hyper_params, single_problem_approach)\u001b[0m\n\u001b[1;32m    184\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39malign(y\u001b[38;5;241m.\u001b[39mdropna(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m X_query \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 187\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_problem_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msingle_problem_approach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m y_output \u001b[38;5;241m=\u001b[39m convert_last_prediction_to_output(y_pred)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_output\n",
      "Cell \u001b[0;32mIn[7], line 217\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(hyper_params, X, y, X_query, single_problem_approach)\u001b[0m\n\u001b[1;32m    215\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining and predicting for all stocks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    216\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhyper_params)\n\u001b[0;32m--> 217\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(model\u001b[38;5;241m.\u001b[39mpredict(X_query), index\u001b[38;5;241m=\u001b[39mX_query\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mcolumns)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gsp-NqpUKMXU-py3.12/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gsp-NqpUKMXU-py3.12/lib/python3.12/site-packages/sklearn/multioutput.py:1155\u001b[0m, in \u001b[0;36mRegressorChain.fit\u001b[0;34m(self, X, Y, **fit_params)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# RegressorChain.base_estimator is not validated yet\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m )\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m        Returns a fitted instance.\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gsp-NqpUKMXU-py3.12/lib/python3.12/site-packages/sklearn/multioutput.py:723\u001b[0m, in \u001b[0;36m_BaseChain.fit\u001b[0;34m(self, X, Y, **fit_params)\u001b[0m\n\u001b[1;32m    721\u001b[0m y \u001b[38;5;241m=\u001b[39m Y[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder_[chain_idx]]\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChain\u001b[39m\u001b[38;5;124m\"\u001b[39m, message):\n\u001b[0;32m--> 723\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_aug\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchain_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m chain_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    730\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m chain_idx\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gsp-NqpUKMXU-py3.12/lib/python3.12/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gsp-NqpUKMXU-py3.12/lib/python3.12/site-packages/xgboost/sklearn.py:1090\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m (\n\u001b[1;32m   1082\u001b[0m     model,\n\u001b[1;32m   1083\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1089\u001b[0m )\n\u001b[0;32m-> 1090\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gsp-NqpUKMXU-py3.12/lib/python3.12/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gsp-NqpUKMXU-py3.12/lib/python3.12/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gsp-NqpUKMXU-py3.12/lib/python3.12/site-packages/xgboost/core.py:2051\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     _check_call(\n\u001b[0;32m-> 2051\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m     )\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_prediction(\n",
    "    run_date=datetime.date(year=2024, month=1, day=1),\n",
    "    name=\"default_multi_approach\",\n",
    "    n_steps=N_STEPS,\n",
    "    days_back_to_consider=DAYS_BACK_TO_CONSIDER,\n",
    "    categorical_features=CATEGORICAL_FEATURES,\n",
    "    label_features=LABEL_FEATURES,\n",
    "    shift_list=SHIFT_LIST,\n",
    "    mwm_list=MWM_LIST,\n",
    "    hyper_params=HYPER_PARAMS,\n",
    "    single_problem_approach=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsp-NqpUKMXU-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
