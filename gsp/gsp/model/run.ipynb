{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, cast, Union, Dict\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from gsp.utils.utils import (\n",
    "    make_mw_in_groups,\n",
    "    make_shift_in_groups,\n",
    "    ColumnTransformerWrapper,\n",
    "    show,\n",
    "    get_nth_previous_working_date,\n",
    ")\n",
    "from data import SCRAPED_STOCK_FILE_PATH, save_output, load_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    return cast(float, mean_squared_log_error(y_true, y_pred) ** 0.5)\n",
    "\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        SCRAPED_STOCK_FILE_PATH,\n",
    "        dtype={\n",
    "            \"Date\": \"period[D]\",\n",
    "            \"Open\": \"float\",\n",
    "            \"High\": \"float\",\n",
    "            \"Low\": \"float\",\n",
    "            \"Close\": \"float\",\n",
    "            \"Volume\": \"int\",\n",
    "            \"Area\": \"category\",\n",
    "            \"Name\": \"category\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy(deep=True)\n",
    "    periods = pd.date_range(\n",
    "        start=df[\"Date\"].min().to_timestamp().date(), end=df[\"Date\"].max().to_timestamp().date(), freq=\"B\"\n",
    "    )\n",
    "    periods_df = pd.DataFrame({\"Date\": periods}, dtype=\"period[D]\")\n",
    "\n",
    "    clean_data = (\n",
    "        cast(\n",
    "            pd.DataFrame,\n",
    "            periods_df.set_index(\"Date\")\n",
    "            .join(df.set_index(\"Date\"))\n",
    "            .set_index(\"Name\", append=True)\n",
    "            .sort_index()\n",
    "            .unstack(\"Name\")\n",
    "            .ffill()\n",
    "            .stack(\"Name\", future_stack=True),  # type: ignore\n",
    "        )\n",
    "        .reset_index()\n",
    "        .dropna(subset=[\"Name\"])\n",
    "        .set_index([\"Date\", \"Name\"])\n",
    "    )\n",
    "\n",
    "    return clean_data\n",
    "\n",
    "\n",
    "def engineer_features(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_features: List[str] = [],\n",
    "    shift_list: List[int] = [],\n",
    "    mwm_list: List[int] = [],\n",
    "    label_features: List[str] = [],\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    cat_features_to_use: List[str] = [*categorical_features, *label_features]\n",
    "\n",
    "    if \"DayOfWeek\" in cat_features_to_use:\n",
    "        df[\"DayOfWeek\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).dayofweek\n",
    "\n",
    "    if \"Month\" in cat_features_to_use:\n",
    "        df[\"Month\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).month\n",
    "\n",
    "    if \"Year\" in cat_features_to_use:\n",
    "        df[\"Year\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).year\n",
    "\n",
    "    if \"WeekOfYear\" in cat_features_to_use:\n",
    "        df[\"WeekOfYear\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).week  # type: ignore\n",
    "\n",
    "    if \"DayOfMonth\" in cat_features_to_use:\n",
    "        df[\"DayOfMonth\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).day\n",
    "\n",
    "    if \"Quarter\" in cat_features_to_use:\n",
    "        df[\"Quarter\"] = cast(pd.PeriodIndex, df.index.get_level_values(\"Date\")).quarter\n",
    "\n",
    "    if \"AreaCat\" in cat_features_to_use:\n",
    "        df[\"AreaCat\"] = df[\"Area\"]\n",
    "\n",
    "    grouped_lags: List[pd.DataFrame | pd.Series] = [\n",
    "        make_shift_in_groups(df, groupby=[\"Name\"], column=\"Close\", shift=shift_list),\n",
    "        make_mw_in_groups(df, groupby=[\"Name\"], column=\"Close\", window=mwm_list),\n",
    "    ]\n",
    "\n",
    "    df_grouped_lags = df.join(grouped_lags, how=\"left\")\n",
    "\n",
    "    return df_grouped_lags\n",
    "\n",
    "\n",
    "def process_data(\n",
    "    df: pd.DataFrame,\n",
    "    n_steps: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str] = [],\n",
    "    days_back_to_consider: int | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    \"\"\"First date where all stocks have data\"\"\"\n",
    "    first_all_valid_date: datetime.date = (\n",
    "        cast(pd.Period, df.unstack(\"Name\").dropna().first_valid_index()).to_timestamp().date()\n",
    "    )\n",
    "\n",
    "    latest_date: datetime.date = cast(pd.Period, df.index.get_level_values(\"Date\").max()).to_timestamp().date()\n",
    "\n",
    "    earliest_date: datetime.date = first_all_valid_date\n",
    "\n",
    "    starting_date_to_consider: datetime.date | None = None\n",
    "\n",
    "    if days_back_to_consider is not None:\n",
    "        starting_date_to_consider = get_nth_previous_working_date(n=days_back_to_consider, date=latest_date)\n",
    "\n",
    "    if starting_date_to_consider is not None and starting_date_to_consider < first_all_valid_date:\n",
    "        show(\n",
    "            f\"WARNING: starting_date_to_consider ({starting_date_to_consider}) is before the first date where all stocks have data. Using {first_all_valid_date} instead.\"\n",
    "        )\n",
    "    elif starting_date_to_consider is not None:\n",
    "        earliest_date = starting_date_to_consider\n",
    "\n",
    "    show(f\"Earliest date: {earliest_date.isoformat()}\")\n",
    "\n",
    "    ctw = ColumnTransformerWrapper(\n",
    "        transformers=[\n",
    "            (\"onehot\", OneHotEncoder(drop=\"first\"), categorical_features),\n",
    "            (\"label\", LabelEncoder(), label_features),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    df_from_earliest = df.loc[earliest_date.isoformat() :]\n",
    "    X = ctw.fit_transform(df_from_earliest.drop(columns=[\"Adj Close\", \"Volume\", \"High\", \"Low\", \"Open\", \"Area\"]))\n",
    "    y = make_shift_in_groups(df, groupby=[\"Name\"], column=\"Close\", shift=[-i for i in range(1, n_steps + 1)])\n",
    "\n",
    "    y, X = y.align(X.dropna(), axis=0, join=\"inner\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    n_steps: int,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"A function that splits the time series data into train and test sets in regards\n",
    "    to the number of forecasting steps. It will test the model on the last 2*n_steps.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Features dataframe\n",
    "        y (pd.DataFrame): Target dataframe\n",
    "        n_steps (int): Number of steps of the forecasting task\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    latest_date = cast(pd.Period, X.index.get_level_values(\"Date\").max()).to_timestamp().date()\n",
    "    test_end_date = get_nth_previous_working_date(n=n_steps, date=latest_date)\n",
    "    test_start_date = get_nth_previous_working_date(n=2 * n_steps - 1, date=latest_date)\n",
    "    train_end_date = get_nth_previous_working_date(n=2 * n_steps, date=latest_date)\n",
    "\n",
    "    X_train = X.loc[: train_end_date.isoformat()]\n",
    "    y_train = y.loc[: train_end_date.isoformat()]\n",
    "    X_test = X.loc[test_start_date.isoformat() : test_end_date.isoformat()]\n",
    "    y_test = y.loc[test_start_date.isoformat() : test_end_date.isoformat()]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def create_model(**hyper_params):\n",
    "    model = RegressorChain(XGBRegressor(**hyper_params))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_and_predict(\n",
    "    model: Union[RegressorChain, MultiOutputRegressor, LinearRegression],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    X_query: pd.DataFrame,\n",
    "):\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_pred = pd.DataFrame(model.predict(X_query), index=X_query.index, columns=y_train.columns)  # type: ignore\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def convert_last_prediction_to_output(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    latest_date = cast(pd.Period, df.index.get_level_values(\"Date\").max()).to_timestamp().date()\n",
    "\n",
    "    y_latest = cast(pd.DataFrame, df.loc[latest_date.isoformat()]).rename(\n",
    "        columns={\n",
    "            f\"Close_lead_{i}\": get_nth_previous_working_date(n=-i, date=latest_date).isoformat()\n",
    "            for i in range(1, len(df.columns) + 1)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    y_output = (\n",
    "        y_latest.reset_index()\n",
    "        .melt(id_vars=[\"Name\"], var_name=\"Date\", value_name=\"Close\")\n",
    "        .astype(\n",
    "            {\n",
    "                \"Date\": \"period[D]\",\n",
    "                \"Name\": \"category\",\n",
    "                \"Close\": \"float\",\n",
    "            }\n",
    "        )\n",
    "        .set_index([\"Date\", \"Name\"])\n",
    "    )\n",
    "\n",
    "    return y_output\n",
    "\n",
    "\n",
    "def optimize_with_optuna(\n",
    "    X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame, n_optimize_trials: int\n",
    ") -> Tuple[Dict, float]:\n",
    "    study = optuna.create_study(direction=\"minimize\", study_name=\"xgboost_optuna\")\n",
    "\n",
    "    def optuna_optimize_objective(trial: optuna.Trial) -> float:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1200, step=50),  # Number of trees in the ensemble\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),  # Maximum depth of each tree\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),  # Learning rate\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),  # Subsample ratio of the training instances\n",
    "            \"colsample_bytree\": trial.suggest_float(\n",
    "                \"colsample_bytree\", 0.5, 1.0\n",
    "            ),  # Subsample ratio of columns when constructing each tree\n",
    "            \"gamma\": trial.suggest_float(\n",
    "                \"gamma\", 0.01, 10.0, log=True\n",
    "            ),  # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 100.0, log=True),  # L1 regularization term on weights\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 100.0, log=True),  # L2 regularization term on weights\n",
    "            \"min_child_weight\": trial.suggest_float(\n",
    "                \"min_child_weight\", 1, 100, log=True\n",
    "            ),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "        }\n",
    "\n",
    "        model = create_model(**params)\n",
    "        y_pred = fit_and_predict(model, X_train, y_train, X_test)\n",
    "        rmsle = root_mean_squared_log_error(y_test, y_pred)\n",
    "\n",
    "        return rmsle\n",
    "\n",
    "    study.optimize(optuna_optimize_objective, n_trials=n_optimize_trials)\n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value\n",
    "\n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "def execute_test_run(\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict = {},\n",
    "    n_optimize_trials: int = -1,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, float, Dict, datetime.date]:\n",
    "\n",
    "    stocks = load_data()\n",
    "    clean = clean_data(stocks)\n",
    "    features = engineer_features(\n",
    "        clean, categorical_features, shift_list=shift_list, mwm_list=mwm_list, label_features=label_features\n",
    "    )\n",
    "    X, y = process_data(features, n_steps, categorical_features, days_back_to_consider=days_back_to_consider)\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, n_steps)\n",
    "    test_prediction_date = cast(pd.Period, X_test.index.get_level_values(\"Date\").max()).to_timestamp().date()\n",
    "    show(X.columns)\n",
    "    show(y.columns)\n",
    "    # --- NOTICE ---\n",
    "    # Default run with given hyperparameters\n",
    "    model_default = create_model(**hyper_params)\n",
    "    y_test_default_pred = fit_and_predict(model_default, X_train, y_train, X_test)\n",
    "    rmsle_default = root_mean_squared_log_error(y_test, y_test_default_pred)\n",
    "\n",
    "    best_params = hyper_params\n",
    "    best_score = rmsle_default\n",
    "\n",
    "    if n_optimize_trials > 0:\n",
    "        opt_params, opt_score = optimize_with_optuna(X_train, y_train, X_test, y_test, n_optimize_trials)\n",
    "\n",
    "        if opt_score < best_score:\n",
    "            best_params = opt_params\n",
    "            best_score = opt_score\n",
    "\n",
    "    show(f\"Best parameters: {best_params}\")\n",
    "    show(f\"Best score: {best_score}\")\n",
    "\n",
    "    # --- NOTICE ---\n",
    "    # Final run with best hyperparameters\n",
    "    opt_model = create_model(**best_params)\n",
    "    y_test_pred = fit_and_predict(opt_model, X_train, y_train, X_test)\n",
    "    rmsle = root_mean_squared_log_error(y_test, y_test_pred)\n",
    "    y_test_output = convert_last_prediction_to_output(y_test_pred)\n",
    "\n",
    "    show(f\"Root Mean Squared Log Error: {rmsle}\")\n",
    "\n",
    "    return features, y_test_output, rmsle, best_params, test_prediction_date\n",
    "\n",
    "\n",
    "def execute_real_run(\n",
    "    n_steps: int,\n",
    "    days_back_to_consider: int,\n",
    "    categorical_features: List[str],\n",
    "    label_features: List[str],\n",
    "    shift_list: List[int],\n",
    "    mwm_list: List[int],\n",
    "    hyper_params: Dict = {},\n",
    ") -> Tuple[pd.DataFrame, datetime.date]:\n",
    "\n",
    "    stocks = load_data()\n",
    "    clean = clean_data(stocks)\n",
    "    features = engineer_features(\n",
    "        clean, categorical_features, label_features=label_features, shift_list=shift_list, mwm_list=mwm_list\n",
    "    )\n",
    "    X, y = process_data(features, n_steps, categorical_features, days_back_to_consider=days_back_to_consider)\n",
    "    X_train, y_train = X.align(y.dropna(), axis=0, join=\"inner\")\n",
    "\n",
    "    prediction_date = cast(pd.Period, X.index.get_level_values(\"Date\").max()).to_timestamp().date()\n",
    "    X_query = X.loc[prediction_date.isoformat() :]\n",
    "\n",
    "    model = create_model(**hyper_params)\n",
    "    y_pred = fit_and_predict(model, X_train, y_train, X_query)\n",
    "    y_output = convert_last_prediction_to_output(y_pred)\n",
    "\n",
    "    return y_output, prediction_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FORECASTING PROBLEM DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps: int = 15\n",
    "n_optimize_trials: int = 50  # --- NOTICE --- Negative value means no optimization\n",
    "days_back_to_consider: int = 5 * 252\n",
    "label_features: List[str] = [\"Year\"]\n",
    "categorical_features: List[str] = [\"DayOfWeek\", \"AreaCat\"]\n",
    "shift_list: List[int] = [1, 2, 3]\n",
    "mwm_list: List[int] = [5, 10, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST EXECUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, y_test_output, rmsle, opt_hyper_params, test_prediction_date = execute_test_run(\n",
    "    n_steps=n_steps,\n",
    "    days_back_to_consider=days_back_to_consider,\n",
    "    categorical_features=categorical_features,\n",
    "    label_features=label_features,\n",
    "    shift_list=shift_list,\n",
    "    mwm_list=mwm_list,\n",
    "    n_optimize_trials=n_optimize_trials,\n",
    ")\n",
    "\n",
    "test_history_log_df = load_output(\"test_history_log.csv\")\n",
    "current_input_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": [test_prediction_date],\n",
    "        \"RMSLE\": [rmsle],\n",
    "        \"HyperParams\": [opt_hyper_params],\n",
    "        \"CategoricalFeatures\": [categorical_features],\n",
    "        \"LabelFeatures\": [label_features],\n",
    "        \"NSteps\": [n_steps],\n",
    "        \"Shift_list\": [shift_list],\n",
    "        \"MWM_list\": [mwm_list],\n",
    "        \"Version\": [len(test_history_log_df) + 1],\n",
    "        \"Phase\": [1],\n",
    "        \"DaysBackToConsider\": [days_back_to_consider],\n",
    "    }\n",
    ").set_index([\"Version\"])\n",
    "\n",
    "test_history_log_df = pd.concat([test_history_log_df, current_input_df], axis=0)\n",
    "\n",
    "save_output(test_history_log_df, \"test_history_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diplay_last_n_days = 150\n",
    "unique_names = y_test_output.index.get_level_values(\"Name\").unique()\n",
    "for name in unique_names:\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    fig.suptitle(name)\n",
    "\n",
    "    cast(pd.DataFrame, features[features.index.get_level_values(\"Name\") == name]).reset_index(\"Name\")[\"Close\"].tail(\n",
    "        diplay_last_n_days\n",
    "    ).plot(label=f\"Close for {name}\", ax=fig.gca())\n",
    "\n",
    "    cast(pd.DataFrame, y_test_output[y_test_output.index.get_level_values(\"Name\") == name]).reset_index(\"Name\")[\n",
    "        \"Close\"\n",
    "    ].plot(label=f\"Predicted Close for {name}\", ax=fig.gca())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REAL EXECUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real_output, prediction_date = execute_real_run(\n",
    "    n_steps=n_steps,\n",
    "    days_back_to_consider=days_back_to_consider,\n",
    "    categorical_features=categorical_features,\n",
    "    label_features=label_features,\n",
    "    shift_list=shift_list,\n",
    "    mwm_list=mwm_list,\n",
    "    hyper_params=opt_hyper_params,\n",
    ")\n",
    "\n",
    "save_output(y_real_output, f\"{prediction_date.isoformat()}_prediction_{n_steps}_steps.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsp-NqpUKMXU-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
